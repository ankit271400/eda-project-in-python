{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load datasets\n",
    "customers = pd.read_csv('Customers.csv')\n",
    "transactions = pd.read_csv('Transactions.csv')\n",
    "products = pd.read_csv('Products.csv')\n",
    "\n",
    "# Merge datasets for unified analysis\n",
    "data = pd.merge(transactions, customers, on='CustomerID', how='left')\n",
    "data = pd.merge(data, products, on='ProductID', how='left')\n",
    "\n",
    "# Initial exploration\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(data.describe())\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Convert TransactionDate and SignupDate to datetime\n",
    "data['TransactionDate'] = pd.to_datetime(data['TransactionDate'])\n",
    "data['SignupDate'] = pd.to_datetime(data['SignupDate'])\n",
    "\n",
    "# Feature engineering: Extract year and month\n",
    "data['TransactionYear'] = data['TransactionDate'].dt.year\n",
    "data['TransactionMonth'] = data['TransactionDate'].dt.month\n",
    "data['SignupYear'] = data['SignupDate'].dt.year\n",
    "\n",
    "# Exploratory Analysis\n",
    "# 1. Distribution of transactions by region\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=data, x='Region', order=data['Region'].value_counts().index)\n",
    "plt.title('Transaction Distribution by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# 2. Revenue contribution by product category\n",
    "category_revenue = data.groupby('Category')['TotalValue'].sum().sort_values(ascending=False)\n",
    "category_revenue.plot(kind='bar', figsize=(10, 6), title='Revenue Contribution by Product Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.show()\n",
    "\n",
    "# 3. Trend of transactions over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_transactions = data.groupby(['TransactionYear', 'TransactionMonth'])['TransactionID'].count()\n",
    "monthly_transactions.plot(title='Monthly Transaction Trends')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.show()\n",
    "\n",
    "# 4. Average transaction value by region\n",
    "region_avg_transaction = data.groupby('Region')['TotalValue'].mean().sort_values(ascending=False)\n",
    "region_avg_transaction.plot(kind='bar', figsize=(10, 6), title='Average Transaction Value by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Average Transaction Value (USD)')\n",
    "plt.show()\n",
    "\n",
    "# 5. Top customers by revenue contribution\n",
    "top_customers = data.groupby('CustomerName')['TotalValue'].sum().sort_values(ascending=False).head(10)\n",
    "top_customers.plot(kind='bar', figsize=(10, 6), title='Top 10 Customers by Revenue Contribution')\n",
    "plt.xlabel('Customer Name')\n",
    "plt.ylabel('Total Revenue (USD)')\n",
    "plt.show()\n",
    "\n",
    "# Derive Business Insights\n",
    "print(\"\\n### Business Insights ###\")\n",
    "print(\"1. The majority of transactions are concentrated in certain regions, with North America leading.\")\n",
    "print(\"2. Product categories significantly impact revenue, with certain categories contributing the majority of the revenue.\")\n",
    "print(\"3. There is a noticeable seasonal trend in transactions, with peaks around certain months.\")\n",
    "print(\"4. Customers from Asia have the highest average transaction value, indicating premium spending behavior.\")\n",
    "print(\"5. The top 10 customers contribute a significant portion of revenue, highlighting the importance of key accounts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load datasets\n",
    "customers = pd.read_csv('Customers.csv')\n",
    "transactions = pd.read_csv('Transactions.csv')\n",
    "products = pd.read_csv('Products.csv')\n",
    "\n",
    "# Merge datasets\n",
    "data = pd.merge(transactions, customers, on='CustomerID', how='left')\n",
    "data = pd.merge(data, products, on='ProductID', how='left')\n",
    "\n",
    "# Aggregate transaction data to create customer profiles\n",
    "customer_profiles = data.groupby('CustomerID').agg({\n",
    "    'TotalValue': ['sum', 'mean'],\n",
    "    'Quantity': 'sum',\n",
    "    'Category': lambda x: x.value_counts().index[0],  # Most frequent category\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "customer_profiles.columns = ['CustomerID', 'TotalSpend', 'AvgSpend', 'TotalQuantity', 'MostFrequentCategory']\n",
    "\n",
    "# Encode categorical data (MostFrequentCategory)\n",
    "customer_profiles = pd.get_dummies(customer_profiles, columns=['MostFrequentCategory'])\n",
    "\n",
    "# Normalize numerical features for similarity calculation\n",
    "scaler = MinMaxScaler()\n",
    "numerical_features = ['TotalSpend', 'AvgSpend', 'TotalQuantity']\n",
    "customer_profiles[numerical_features] = scaler.fit_transform(customer_profiles[numerical_features])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "customer_features = customer_profiles.drop('CustomerID', axis=1)\n",
    "similarity_matrix = cosine_similarity(customer_features)\n",
    "\n",
    "# Recommendation function\n",
    "def get_similar_customers(customer_id, top_n=3):\n",
    "    # Get the index of the customer in the matrix\n",
    "    customer_idx = customer_profiles[customer_profiles['CustomerID'] == customer_id].index[0]\n",
    "    \n",
    "    # Get similarity scores for the customer\n",
    "    similarity_scores = similarity_matrix[customer_idx]\n",
    "    \n",
    "    # Sort by similarity score\n",
    "    similar_customers_idx = similarity_scores.argsort()[::-1][1:top_n+1]\n",
    "    \n",
    "    # Retrieve similar customers and their scores\n",
    "    similar_customers = customer_profiles.iloc[similar_customers_idx][['CustomerID']].copy()\n",
    "    similar_customers['SimilarityScore'] = similarity_scores[similar_customers_idx]\n",
    "    \n",
    "    return similar_customers\n",
    "\n",
    "# Generate recommendations for the first 20 customers\n",
    "lookalike_results = {}\n",
    "for customer_id in customer_profiles['CustomerID'].head(20):\n",
    "    similar_customers = get_similar_customers(customer_id)\n",
    "    lookalike_results[customer_id] = similar_customers.values.tolist()\n",
    "\n",
    "# Save results to CSV\n",
    "lookalike_df = pd.DataFrame({\n",
    "    'CustomerID': lookalike_results.keys(),\n",
    "    'SimilarCustomers': lookalike_results.values()\n",
    "})\n",
    "lookalike_df.to_csv('Lookalike.csv', index=False)\n",
    "\n",
    "# Display sample recommendations\n",
    "print(lookalike_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load datasets\n",
    "customers = pd.read_csv('Customers.csv')\n",
    "transactions = pd.read_csv('Transactions.csv')\n",
    "\n",
    "# Merge datasets\n",
    "data = pd.merge(transactions, customers, on='CustomerID', how='left')\n",
    "\n",
    "# Aggregate transaction data to create customer profiles\n",
    "customer_profiles = data.groupby('CustomerID').agg({\n",
    "    'TotalValue': ['sum', 'mean'],  # Total and Average Spend\n",
    "    'Quantity': 'sum',             # Total Quantity Purchased\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "customer_profiles.columns = ['CustomerID', 'TotalSpend', 'AvgSpend', 'TotalQuantity']\n",
    "\n",
    "# Merge with customer profile information (Region, SignupDate)\n",
    "customer_profiles = pd.merge(customer_profiles, customers[['CustomerID', 'Region']], on='CustomerID', how='left')\n",
    "\n",
    "# Encode categorical data (Region)\n",
    "encoder = OneHotEncoder()\n",
    "encoded_region = encoder.fit_transform(customer_profiles[['Region']]).toarray()\n",
    "encoded_region_df = pd.DataFrame(encoded_region, columns=encoder.get_feature_names_out(['Region']))\n",
    "customer_profiles = pd.concat([customer_profiles, encoded_region_df], axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "customer_profiles = customer_profiles.drop(['CustomerID', 'Region'], axis=1)\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "customer_profiles_scaled = scaler.fit_transform(customer_profiles)\n",
    "\n",
    "# Clustering with K-Means\n",
    "db_scores = []\n",
    "cluster_range = range(2, 11)\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    clusters = kmeans.fit_predict(customer_profiles_scaled)\n",
    "    db_index = davies_bouldin_score(customer_profiles_scaled, clusters)\n",
    "    db_scores.append(db_index)\n",
    "\n",
    "# Find optimal number of clusters (minimum DB Index)\n",
    "optimal_k = cluster_range[np.argmin(db_scores)]\n",
    "print(f\"Optimal Number of Clusters: {optimal_k}\")\n",
    "\n",
    "# Perform clustering with optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "customer_profiles['Cluster'] = kmeans.fit_predict(customer_profiles_scaled)\n",
    "\n",
    "# Visualize DB Index\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, db_scores, marker='o', linestyle='-', color='b')\n",
    "plt.title('Davies-Bouldin Index for Different Cluster Numbers')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('DB Index')\n",
    "plt.show()\n",
    "\n",
    "# Visualize clusters using PCA\n",
    "pca = PCA(n_components=2)\n",
    "customer_profiles_pca = pca.fit_transform(customer_profiles_scaled)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=customer_profiles_pca[:, 0], y=customer_profiles_pca[:, 1], hue=customer_profiles['Cluster'], palette='Set2', s=100)\n",
    "plt.title('Customer Segmentation Clusters (PCA Projection)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate and report metrics\n",
    "db_index_optimal = davies_bouldin_score(customer_profiles_scaled, customer_profiles['Cluster'])\n",
    "print(f\"Davies-Bouldin Index for {optimal_k} Clusters: {db_index_optimal:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
